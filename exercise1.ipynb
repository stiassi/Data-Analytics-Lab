{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7627855d",
   "metadata": {},
   "source": [
    "# Data Sources\n",
    "\n",
    "\n",
    "1) Wikimedia Dump: https://meta.wikimedia.org/wiki/Data_dumps. Das sind die offiziellen XML Dumps von Wikimedia, die monatlich aktualisiert werden. Wie auf der Website angegeben, sind die Datensätze ziemlich groß – etwa 20 GB für Englisch und 8 GB für Deutsch. Die Verarbeitung wäre also ziemlich aufwendig, da man erst einmal jede Menge XML Daten bereinigen müsste. Außerdem bräuchte man zum Extrahieren des Inhalts zusätzliche Tools oder Bibliotheken. Die Dumps sind zwar die vollständigste Datenquelle, aber für eine Uni-Aufgabe wahrscheinlich zu komplex.\n",
    "\n",
    "\n",
    "2) Wikimedia Dataset: https://huggingface.co/datasets/wikimedia/wikipedia. Dieser Datensatz basiert auf den Wikipedia-Dumps und enthält bereinigte Artikel in vielen Sprachen. Das wäre dann perfekt, um die Abdeckung von englischen und deutschen Artikeln miteinander zu vergleichen. Die Daten stammen aus 2023, sind also nicht ganz aktuell. Auch wenn die Artikel schon aufbereitet wurden, ist nicht ganz klar, ob weiterhin \"non-entity pages\" enthalten sind. Dafür ist der Datensatz einfach zu verarbeiten, weil es für jede Sprache einen eigenen Teil-Datensatz gibt.\n",
    "\n",
    "3) Wiki40b: https://www.tensorflow.org/datasets/catalog/wiki40b. Dieser Datensatz wurde stark bereinigt. Er enthält englische und deutsche Artikel und eignet sich grundsätzlich gut für den Vergleich. Der Nachteil: Da Wiki40b ursprünglich für Machine-Learning-Zwecke gedacht ist, fehlen vermutlich einige nützliche Inhalte, was die Vollständigkeit einschränkt.\n",
    "\n",
    "\n",
    "Bei meiner Recherche wollte ich einen Datensatz finden, der aktuell, vollständig und in beiden Sprachen (Englisch und Deutsch) verfügbar ist. Das Wikimedia Dataset auf Hugging Face hat sich als beste Wahl herausgestellt, weil es sich einfach verarbeiten lässt und trotzdem genug Inhalte bietet.\n",
    "\n",
    "Beim Arbeiten mit dem Hugging-Face-Datensatz ist mir aufgefallen, dass ich anfangs gar nicht genau überlegt hatte, wie ich gezielt Artikel zum Thema „Algorithmen“ finden kann. In diesen Datensätzen gibt es keinen direkten Filter dafür, und das bloße Suchen nach dem Wort „Algorithm“ im Titel oder Text führte zu viel zu vielen unpassenden Treffern (viele Artikel hatten mit dem Thema gar nichts zu tun). Im Endeffekt waren die oben beschriebenen Datensätze nicht wirklich hilfereich und ich musste nach was anderem suchen...\n",
    "\n",
    "\n",
    "Deshalb habe ich mich letztendlich für die API von **query.wikidata.org/sparql** entschieden. Die Daten stammen von Wikidata Central Database und werden in Echtzeit abgerufen. Damit könnte man sehr gut nach Entitäten, Eigenschaften, Sprachen usw. filtern und kombinierte Abfragen erstellen (was bei der späteren Analyse sehr praktisch war). Die Nutzung der API machte die Arbeit viel effizienter, da keine vollständigen Datensätze geladen, sondern nur die tatsächlich benötigten Informationen abgefragt wurden.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c887f21",
   "metadata": {},
   "source": [
    "## Alle Algorithmen in der englischen Wikipedia. \n",
    "\n",
    "Diese Liste enthält keine Duplikate. Algorithmen ohne Beschreibung sind nicht enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4916fb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1453\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "wiki_api = \"https://query.wikidata.org/sparql\"\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?algorithm ?description WHERE {\n",
    "  ?class wdt:P279+ wd:Q8366.\n",
    "  ?algorithm wdt:P31 ?class.\n",
    "  ?algorithm schema:description ?description.\n",
    "  FILTER(LANG(?description) = \"en\" && STRLEN(?description) > 0)\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"here was my Email Address\",\n",
    "    \"Accept\": \"application/sparql-results+json\"\n",
    "}\n",
    "try:\n",
    "    output = requests.get(wiki_api, params={'query': query}, headers=headers)\n",
    "\n",
    "    data = output.json()\n",
    "    \n",
    "    algorithms = []\n",
    "\n",
    "    for result in data[\"results\"][\"bindings\"]:\n",
    "        article = result[\"algorithm\"][\"value\"]\n",
    "        algorithms.append(article)\n",
    "\n",
    "    print(len(algorithms))\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eab2d9",
   "metadata": {},
   "source": [
    "## Alle Algorithmen in der deutschen Wikipedia. \n",
    "\n",
    "Diese Liste enthält keine Duplikate. Algorithmen ohne Beschreibung sind nicht enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b164291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "wiki_api = \"https://query.wikidata.org/sparql\"\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?algorithm ?description WHERE {\n",
    "  ?class wdt:P279+ wd:Q8366.\n",
    "  ?algorithm wdt:P31 ?class.\n",
    "  ?algorithm schema:description ?description.\n",
    "\n",
    "  FILTER(LANG(?description) = \"de\" && STRLEN(?description) > 0)\n",
    "  \n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"de\". }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"here was my Email Address\",\n",
    "    \"Accept\": \"application/sparql-results+json\"\n",
    "}\n",
    "try:\n",
    "    output = requests.get(wiki_api, params={'query': query}, headers=headers)\n",
    "    \n",
    "    data = output.json()\n",
    "    \n",
    "    algorithms = []\n",
    "\n",
    "    for result in data[\"results\"][\"bindings\"]:\n",
    "        article = result[\"algorithm\"][\"value\"]\n",
    "        algorithms.append(article)\n",
    "\n",
    "    print(len(algorithms))\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb601a",
   "metadata": {},
   "source": [
    "# Überprüfung fehlender Einträge\n",
    "\n",
    "Zunächst war mein Ansatz, nach Einträgen zu suchen, die sowohl einen deutschen als auch einen englischen Titel besitzen. Das war aber nicht so sinnvoll, da manche deutschen Artikel denselben englischen Titel verwenden.\n",
    "\n",
    "Auch die Suche nach einer deutschen Beschreibung war keine gute Idee, weil viele Beschreibungen leer sind oder lediglich das englische Wort wiederholen.\n",
    "\n",
    "Danach habe ich versucht, den entsprechenden Eintrag in der jeweils anderen Wikipedia zu finden und das hat einwandfrei funktioniert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f5616",
   "metadata": {},
   "source": [
    "## Einträge die in der englischen Wikipedia, aber nicht in der deutschen enthalten sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175b22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543\n",
      "['http://www.wikidata.org/entity/Q2658354', 'http://www.wikidata.org/entity/Q2662711', 'http://www.wikidata.org/entity/Q2670506', 'http://www.wikidata.org/entity/Q2679145', 'http://www.wikidata.org/entity/Q2704282', 'http://www.wikidata.org/entity/Q2709102', 'http://www.wikidata.org/entity/Q2719731', 'http://www.wikidata.org/entity/Q2734005', 'http://www.wikidata.org/entity/Q2735449', 'http://www.wikidata.org/entity/Q2739975']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "wiki_api = \"https://query.wikidata.org/sparql\"\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?algorithm ?enArticle WHERE {\n",
    "  ?class wdt:P279+ wd:Q8366.\n",
    "  ?algorithm wdt:P31 ?class.\n",
    "\n",
    "  ?algorithm schema:description ?description.\n",
    "  FILTER(STRLEN(?description) > 0)\n",
    "\n",
    "  ?enArticle schema:about ?algorithm .\n",
    "  FILTER(CONTAINS(STR(?enArticle), \"en.wikipedia.org\"))\n",
    "\n",
    "  FILTER NOT EXISTS {\n",
    "    ?deArticle schema:about ?algorithm .\n",
    "    FILTER(CONTAINS(STR(?deArticle), \"de.wikipedia.org\"))\n",
    "  }\n",
    "\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"here was my Email Address\",\n",
    "    \"Accept\": \"application/sparql-results+json\"\n",
    "}\n",
    "try:\n",
    "    output = requests.get(wiki_api, params={'query': query}, headers=headers)\n",
    "\n",
    "    data = output.json()\n",
    "    \n",
    "    algorithms = []\n",
    "\n",
    "    for result in data[\"results\"][\"bindings\"]:\n",
    "        article = result[\"algorithm\"][\"value\"]\n",
    "        algorithms.append(article)\n",
    "\n",
    "    print(len(algorithms))\n",
    "    print(algorithms[:10])\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72abbd8e",
   "metadata": {},
   "source": [
    "## Einträge die in der deutschen Wikipedia, aber nicht in der englischen enthalten sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21b07d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "['http://www.wikidata.org/entity/Q1486014', 'http://www.wikidata.org/entity/Q1530301', 'http://www.wikidata.org/entity/Q245439', 'http://www.wikidata.org/entity/Q338522', 'http://www.wikidata.org/entity/Q603110', 'http://www.wikidata.org/entity/Q965243', 'http://www.wikidata.org/entity/Q108042034', 'http://www.wikidata.org/entity/Q109418712', 'http://www.wikidata.org/entity/Q114961711', 'http://www.wikidata.org/entity/Q118904446']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "wiki_api = \"https://query.wikidata.org/sparql\"\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?algorithm ?deArticle WHERE {\n",
    "  ?class wdt:P279+ wd:Q8366.\n",
    "  ?algorithm wdt:P31 ?class.\n",
    "  ?algorithm schema:description ?description.\n",
    "\n",
    "  ?deArticle schema:about ?algorithm .\n",
    "  FILTER(CONTAINS(STR(?deArticle), \"de.wikipedia.org\"))\n",
    "\n",
    "  FILTER NOT EXISTS {\n",
    "    ?enArticle schema:about ?algorithm .\n",
    "    FILTER(CONTAINS(STR(?enArticle), \"en.wikipedia.org\"))\n",
    "  }\n",
    "\n",
    "  FILTER(STRLEN(?description) > 0)\n",
    "\n",
    "\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"de\". }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"here was my Email Address\",\n",
    "    \"Accept\": \"application/sparql-results+json\"\n",
    "}\n",
    "try:\n",
    "    output = requests.get(wiki_api, params={'query': query}, headers=headers)\n",
    "\n",
    "    data = output.json()\n",
    "    \n",
    "    algorithms = []\n",
    "\n",
    "    for result in data[\"results\"][\"bindings\"]:\n",
    "        article = result[\"algorithm\"][\"value\"]\n",
    "        algorithms.append(article)\n",
    "\n",
    "    print(len(algorithms))\n",
    "    print(algorithms[:10])\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error:\", e) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100c616",
   "metadata": {},
   "source": [
    "# Quality & Quantity\n",
    "\n",
    "Es gibt 543 Einträge in der englischen Wikipedia, die nicht in der deutschen vorhanden sind.\n",
    "Und 47 Einträge in der deutschen Wikipedia, die nicht in der englischen zu finden sind.\n",
    "\n",
    "Auch bei einer manuellen Überprüfung konnte ich einige der fehlenden Artikel nicht finden. Manchmal sind die entsprechenden Informationen lediglich als Unterabschnitt auf einer anderen Seite zu finden. So gibt es beispielsweise in der deutschen Wikipedia einen eigenen Artikel über Quadrupelbauer (https://de.wikipedia.org/wiki/Quadrupelbauer), während das Thema in der englischen Wikipedia nur als Unterabschnitt auf der Seite über Doubled Pawns (https://en.wikipedia.org/wiki/Doubled_pawns) behandelt wird.\n",
    "\n",
    "Daher halte ich diesen Ansatz für eine gute Basis-Lösung.\n",
    "\n",
    "Jedoch sehe ich ein anderes Problem mit Quality. Einige Artikel, die als „Algorithmen“ markiert sind, würde ich nicht wirklich so bezeichnen. ( z.B. Quadrupelbauer)\n",
    "Das ist aber eher eine subjektive Einschätzung und hängt vom Analyseziel ab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817be63e",
   "metadata": {},
   "source": [
    "# Action Plan\n",
    "\n",
    "Um Verwirrungen zu vermeiden und Coverage zu verbessern sollte die Kategorisierung von Algorithmus-Artikeln klarer definiert werden. (Wann gehört ein Artikel in die Kategorie \"Algorithmus\"? Was sind die Kriterien?). Danach könnte man zB (halb)automatische Tools einsetzen, um fehlende Artikel zu identifizieren.\n",
    "\n",
    "Um Coverage zu verbessern sollten Teilabdeckungen berücksichtigt werden und nicht nur vollständig übereinstimmende Artikel. Ein Ansatz, der solche Abschnittsüberschneidungen erkennen kann, würde die Analyse genauer machen.\n",
    "\n",
    "Einige Unterschiede entstehen vermutlich, weil Wikidata-Verknüpfungen fehlen oder unvollständig sind. In meiner Analyse habe ich z.B. die Einträge rausgefiltert, deren Beschreibung leer ist. Daher sollte man die Links zwischen der englischen und deutschen Wikipedia prüfen und ergänzen, sowie Beschreibungen hinzufügen (es macht meiner Meining keinen Sinn, wenn die Seite einfach nur da ist, aber gar keine nützlichen Infos enthält).\n",
    "\n",
    "Um die Lücken in der Coverage zu schließen, sollte man die Übersetzung englischer Artikel ins Deutsche (und umgekehrt) fördern.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
